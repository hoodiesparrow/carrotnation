## ğŸ“° ë¹…ë°ì´í„° ë¶„ì„ ì‚¬ì „í•™ìŠµ 1ê°•



#### MapReduce Programing Model

-  í•¨ìˆ˜í˜• í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì˜ í˜•íƒœ
- ìœ ì €ëŠ” ì•„ë˜ 3ê°€ì§€ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ì„œ ì œê³µ
  - Main í•¨ìˆ˜
  - Map í•¨ìˆ˜ (key1, val1)  ->  [(key2, val2)] 
  - Reduce í•¨ìˆ˜ (key2, [val2])  ->  [(key3, val3)] 



#### MapReduce Phase (3ë‹¨ê³„)

- ë§µ(Map) í˜ì´ì¦ˆ

  - ì œì¼ ë¨¼ì €ìˆ˜í–‰ë˜ë©° ë°ì´í„°ì˜ ì—¬ëŸ¬ íŒŒí‹°ì…˜ì— ë³‘ë ¬ ë¶„ì‚°ìœ¼ë¡œ í˜¸ì¶œë˜ì–´ ìˆ˜í–‰ëœë‹¤.
  - ê° ë¨¸ì‹ ë§ˆë‹¤ ìˆ˜í–‰ëœ MapperëŠ” ë§µ í•¨ìˆ˜ê°€ ì…ë ¥ ë°ì´í„°ì˜ í•œ ì¤„ ë§ˆë‹¤ ë§µ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•œë‹¤.
  - ë§µ í•¨ìˆ˜ëŠ” (key, value)ìŒ í˜•íƒœë¡œ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ê³  ì—¬ëŸ¬ ë¨¸ì‹ ì— ë‚˜ëˆ„ì–´ ë³´ë‚´ë©´ ê°™ì€ keyë¥¼ ê°€ì§„ (key, value) ìŒì€ ê°™ì€ ë¨¸ì‹ ìœ¼ë¡œ ë³´ë‚´ì§„ë‹¤.

- ì…”í”Œë§(Shuffling) í˜ì´ì¦ˆ

  - ëª¨ë“  ë¨¸ì‹ ì—ì„œ ë§µ í˜ì´ì¦ˆê°€ ë‹¤ ëë‚˜ë©´ ì‹œì‘ëœë‹¤.

  - ë§µ í˜ì´ì¦ˆì—ì„œ ê°ê°ì˜ ë¨¸ì‹ ìœ¼ë¡œ ë³´ë‚´ì§„ (key, value) ìŒì„ KEYë¥¼ ì´ìš©í•´ì„œ ì •ë ¬ì„ í•œ í›„

    ê°ê°ì˜ KEY ë§ˆë‹¤ ê°™ì€ KEYë¥¼ ê°€ì§„ (key, value) ìŒì„ ëª¨ì•„ì„œ ë°¸ë¥˜-ë¦¬ìŠ¤íŠ¸(value-list)ë¥¼ ë§Œë“  ë‹¤ìŒì— 

    (key, value-list)í˜•íƒœë¡œ KEYì— ë”°ë¼ì„œ ì—¬ëŸ¬ ë¨¸ì‹ ì— ë¶„ì‚°í•´ì„œ ë³´ë‚¸ë‹¤.

- ë¦¬ë“€ìŠ¤(Reduce) í˜ì´ì¦ˆ

  - ëª¨ë“  ë¨¸ì‹ ì—ì„œ ì…”í”Œë§ í˜ì´ì¦ˆê°€ ë‹¤ ëë‚˜ë©´ ê° ë¨¸ì‹ ë§ˆë‹¤ ë¦¬ë“€ìŠ¤ í˜ì´ì¦ˆê°€ ì‹œì‘ëœë‹¤.

  - ê°ê°ì˜ ë¨¸ì‹ ì—ì„œëŠ” ì…”í”Œë§ í˜ì´ì¦ˆì—ì„œ í•´ë‹¹ ë¨¸ì‹ ìœ¼ë¡œ ë³´ë‚´ì§„ ê°ê°ì˜ (key, value-list) ìŒ ë§ˆë‹¤ ë¦¬ë“€ìŠ¤ í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ë©°

    í•˜ë‚˜ì˜ ë¦¬ë“€ìŠ¤ í•¨ìˆ˜ê°€ ëë‚˜ë©´ ë‹¤ìŒ (key, value-list) ìŒ ë¦¬ë“€ìŠ¤ í•¨ìˆ˜ê°€ í˜¸ì¶œ

  - ì¶œë ¥ì´ ìˆë‹¤ë©´ (key, value) ìŒ í˜•íƒœë¡œ ì¶œë ¥



![hadoopMapReduce](hadoopMapReduce.png)



#### MapReduce ì…ì¶œë ¥ Default í´ë˜ìŠ¤

- í•˜ë‘¡ì˜ ë§µë¦¬ë“€ìŠ¤ì˜ ë§µí•¨ìˆ˜, ë¦¬ë“€ìŠ¤í•¨ìˆ˜, ì»´ë°”ì¸ í•¨ìˆ˜ ë“±ì—ì„œ ì…ì¶œë ¥ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í´ë˜ìŠ¤ì™€ í•´ë‹¹ë˜ëŠ” ìë°” íƒ€ì…
  - Text : String
  - IntWritable : int
  - LongWritable : long
  - FloatWritable : float
  - DoubleWritable : double 
- ë§Œì¼ ìƒˆë¡œìš´ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•´ì„œ ì…ì¶œë ¥ì— ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ í•„ìš”í•œ ì—¬ëŸ¬ í•¨ìˆ˜ë„ í•¨ê»˜ ì •ì˜ë¥¼ ê°™ì´ í•´ì£¼ì–´ì•¼ë§Œ í•œë‹¤.





#### Combine í•¨ìˆ˜

- ë¦¬ë“€ìŠ¤ í•¨ìˆ˜ì™€ ìœ ì‚¬í•œ í•¨ìˆ˜ì¸ë° ê° ë¨¸ì‹ ì—ì„œ ë§µ í˜ì´ì¦ˆì—ì„œ ë§µ í•¨ìˆ˜ì˜ ì¶œë ¥ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ 

  ì…”í”Œë§ í˜ì´ì¦ˆì™€ ë¦¬ë“€ìŠ¤ í˜ì´ì¦ˆì˜ ë¹„ìš©ì„ ì¤„ì—¬ì£¼ëŠ”ë° ì‚¬ìš©

```java
public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs();
		if ( otherArgs.length != 2 ) {
			System.err.println("Usage: <in> <out>");
			System.exit(2);
		}
		Job job = new Job(conf,"word count");
		//job
		job.setJarByClass(Wordcount1char.class);

		// let hadoop know my map and reduce classes
		job.setMapperClass(TokenizerMapper.class);
    	// Reduce í•¨ìˆ˜ë¥¼ ë§µ í˜ì´ì¦ˆ ëë‚˜ê³  ìˆ˜í–‰]
    	job.setCombinerClass(IntSumReducer.class); 		// Combiner class ì„ ì–¸
		job.setReducerClass(IntSumReducer.class);
		
		job.setOutputKeyClass(Text.class);				//Output key typeì„ ì–¸
		job.setOutputValueClass(IntWritable.class);		//Output value typeì„ ì–¸
		//job.setMapOutputKeyClass(Text.class);					//Map is different from Reducer fnc output key type
		//job.setMapOutputValueClass(IntWritable.class);		//Map is different from Reducer fnc output value type

		// set number of reduces
		job.setNumReduceTasks(2);

		// set input and output directories
		FileInputFormat.addInputPath(job,new Path(otherArgs[0]));
		FileOutputFormat.setOutputPath(job,new Path(otherArgs[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1 );
	}

```

